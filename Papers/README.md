# To Read

* [] 1997, Hochreiter: Long short-term memory
* [] 2002, Papineni: A method for automatic evaluation of machine translation
* [] 2003, Bengio: A neural probabilistic language model
* [] 2006, Graves: Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks
* [] 2008, Hinton: Visualizing data using t-SNE
* [] 2013, Lee: Network in Network
* [] 2013, Girshik: Rich feature hierarchies for accurate object detection and semantic segmentation
* [] 2013, Zeiler: Visualizing and understanding convolutional networks
* [] 2013, Mikolov: Linguistic regularities in continuous space word representations
* [] 2013, Mikolov: Efficient estimation of word representations in vector space
* [] 2013, Mikolov: Distributed representation of words and phrases and their compositionality
* [] 2014, Pennington: Glove Global Vectors for word representation
* [] 2014: Going Deeper with Convolutions (GooLeNet)
* [] 2014, Sermamet: OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks
* [] 2014, Taigman: DeepFace closing the gap to human level performance
* [] 2014, Cho: On the properties of neural machine translation. Encoder-decoder approaches
* [] 2014, Chung: Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modelling
* [] 2014, Sutskever: Sequence to sequence learning with neural networks
* [] 2014, Cho: Learning phrase representations using RNN encoder-decoder for statistical machine translation
* [] 2014, Mao: Deep captioning with multimodal recurrent neural networks
* [] 2014, Vinyals: Show and tell Neural Image caption generator
* [] 2014, Bahdanaeu: Neural machine translation by jointly learning to align and translate
* [] 2015, Xu: Show, Attend and Tell: Neural Image Caption Generation with Visual Attention
* [] 2015, Szegedy: Rethinking the Inception Architecture for Computer Vision
* [x] 2015: Deep Residual Networks for Image Recognition
* [] 2015, Redmon: You Only Look Once: Unified, Real-Time Object Detection
* [] 2015, Ronneberger: U-Net: Convolutional Networks for Biomedical Image Segmentation
* [] 2015, Schroff: FaceNet: A unified embedding for face recognition and clustering
* [] 2015, Gatys: A neural algorithm of artistic style.
* [] 2015, Karphaty: Deep visual-semantic alignments for generating image descriptions
* [] 2016, Szegedy: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning
* [] 2016, Bolukbasi: Man is to computer programming as woman is to homemaker? Debiasing word embeddings
* [] 2016, Huang: Densely Connected Convolutional Networks
* [] 2017, Xie: Aggregated Residual Transformations for Deep Neural Networks
* [] 2017, Zagoruyko: Wide Residual Networks
* [] 2017, Hu: Squeeze-and-Excitation Networks
* [] 2017, Howard: MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications
* [] 2017, Novikov: Fully Convolutional Architectures for Multi-Class Segmentation in Chest Rediographs
* [] 2017, Dong: Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks
* [] 2017, Vaswani: Attention is All You Need
* [] 2018, Woo: CBAM: Convolutional Block Attention Module
* [] 2018, Woo: CBAM: Convolutional Block Attention Module
* [] 2018, Zhang: Residual Attention Network for Image Classification
* [] 2019, Tan: EfficientNet, Rethinking Model Scoring for Convolutional Neural Networks
* [] 2019, Sandler: MobileNetV2, Inverted Residuals and Linear Bottlenecks
* [] 2020, Carion: End-to-End Object Detection with Transformers
* [] 2020, Brown: Language Models are Few-Shot Learners
* [] 2020, Kaplan: Scaling Laws for Neural Language Models
* [] 2020, Stiennon: Learning to summarize from human feedback
* [] 2020, Lewis: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
* [] 2021, Hu: LoRA: Low-Rank Adaptation of Large Language Models
* [] 2021, Lester: The Power of Scale for Parameter-Efficient Prompt Tuning
* [] 2022, Hoffmann: Training Computer-Optimal Large Language Models
* [] 2022, Won: Scaling Instruction-Finetuned Language Models
* [] 2022, Bai: Constitutional AI Harmlessness from AI Feedback
* [] 2022, Wei: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
* [] 2022, Gao: PAL Program-aided Language Models
* [] 2022, Yao: ReAct Synergizing Reasoning and Acting in Language Models
* [] 2023, Touvron: LLaMA Open and Efficient Foundation Language Model
* [] 2023, Wu: BloombergGPT A Large Language Model for Finance
* [] 2023, Lialin: Scaling Down to Scale Up A Guide to Parameter-Efficient Fine-Tunning